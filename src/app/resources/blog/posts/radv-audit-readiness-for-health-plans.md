# RADV Audit Readiness for Health Plans | Martlet AI

**Date:** 12.15.2025  
**Thumbnail:** /assets/blog/radv-audit-readiness-for-health-plans.png  
**Description:** Strengthen RADV audit readiness with Martlet AI's AI-driven workflows, reducing financial risks and improving documentation accuracy for health plans.  
**Tag:** Product

# RADV Audit Readiness for Health Plans: Reducing Clawback Risk and Strengthening Compliance with AI-Driven Workflows

**How confident are you that a RADV sample would not pick up a few missing documents and then use those errors to estimate losses across your entire contract?**

This is the financial risk that keeps Medicare leaders up at night: a small mistake in the sample can snowball into a repayment measured in the millions.

When proof of illness is scattered across different systems and takes too long to pull together, it becomes harder to show auditors the evidence that would prevent those sample errors from being applied more broadly.

This guide explains how health plans can lower that risk and build stronger RADV audit readiness by using AI-driven workflows to organize evidence faster and more reliably.

## Why Clawback Exposure is Rising and Why Health Plans Need Early Visibility

Health plan leaders can manage coding and operations, but RADV exposure is harder to control because the biggest risks often only appear once auditors review the file. That is why understanding how exposure forms in the first place has become so important.

### Extrapolation Raises the Financial Stakes

The core risk in today’s RADV audits is that a single documentation miss in the sample can shape CMS’s view of the entire contract. In its [July 2025 guidance for payment year 2018 RADV audits](https://www.cms.gov/files/document/payment-year-2018-ma-radv-audit-methods-instructions.pdf), CMS states that extrapolation is expected to be standard practice for Medicare Advantage contract reviews. As a result, a limited number of unsupported diagnoses identified in the sample can drive a much larger contract-level overpayment estimate

### Audit Expansion Increases Operational Pressure

CMS is also broadening its audit reach. It plans to [review every eligible Medicare Advantage contract](https://www.cms.gov/newsroom/press-releases/cms-rolls-out-aggressive-strategy-enhance-and-accelerate-medicare-advantage-audits) and is working with the Department of Health and Human Services Office of Inspector General to recover past overpayments. This raises the bar for documentation, which must be:

*   complete when the chart is pulled
*   consistent across systems
*   clear enough for auditors to verify quickly

The faster audit cadence leaves plans with less time to gather charts, fix small documentation issues, or clarify the clinical story before those records are pulled into the RADV sample.

### Unsupported Diagnoses Heighten Sample Risk

[Recent findings from the Office of Inspector General](https://oig.hhs.gov/reports/all/2024/medicare-advantage-questionable-use-of-health-risk-assessments-continues-to-drive-up-payments-to-plans-by-billions/) highlight that many diagnoses appear only on health risk assessments or on chart reviews tied to those assessments. However, there is no sign of these conditions in the service records that auditors rely on.

The review found that unsupported diagnoses resulted in $7.5 billion in Medicare Advantage payments in 2023. Therefore, if a diagnosis does not appear in any follow-up visit, test, or treatment record, it raises questions about its accuracy. Moreover, it becomes harder for a plan to defend that condition if it appears in the RADV sample.

## Where Current Workflows Create Compliance Gaps and Clawback Risk

Organizations are exposed to clawback risks when everyday workflows make it hard to produce the evidence that RADV auditors expect.

### Fragmented Records Make Evidence Hard to Locate

A member’s history may [sit across several EHR systems](https://pmc.ncbi.nlm.nih.gov/articles/PMC7731898/), scanned documents, and unstructured notes, much of it not searchable. [Research shows](https://pubmed.ncbi.nlm.nih.gov/39932774/) that navigating multi-source records slows review and increases the chances that key information is missed. When evidence exists but cannot be surfaced quickly, a diagnosis that should be defensible can appear unsupported in the RADV sample.

### Thin Documentation and Coding Drift Leave Evidence Incomplete

OIG reviews [continue to find diagnoses without full clinical support](https://oig.hhs.gov/reports/all/2023/high-rates-of-prior-authorization-denials-by-some-plans-and-limited-state-oversight-raise-concerns-about-access-to-care-in-medicaid-managed-care/) in the medical record, which has increased auditor attention to how clearly each condition is documented.

Diagnoses that lack clear assessment, monitoring or treatment often appear incomplete to reviewers and are more likely to be removed in RADV. These issues surface as HCCs that lack enough documentation to back the code, making them more likely to be removed during RADV.

### Intake Failures and Manual Bottlenecks Jeopardize Submission Validity

CMS reviewer guidance allows records to be excluded when dates, signatures or credentials are missing or unclear. These intake issues can remove legitimate diagnoses before auditors consider the clinical context. [Research shows](https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2024.1475092/full) that reviewing unstructured EHR data can take up to 30 minutes per case, and information overload limits what reviewers can reliably catch.

Experts who spend more time on manual steps have less time to assemble packets carefully, which increases the risk that required documents are missing. These risks put defensible diagnoses at risk during RADV review.

## How Martlet AI Transforms RADV Workflows Into Defensible Evidence

Health plans cannot change how CMS samples or extrapolates, but they can change how clearly they see their own evidence before that happens. The question is whether a plan can, at any point in the year, see how a RADV reviewer would read its charts and which diagnoses would be hardest to defend.

Martlet AI is designed around that practical need, turning fragmented records and manual checks into a more consistent, evidence-first workflow.

### Unify Charts for Contract-Level Review

Martlet AI enables plans to pull structured EHR data, claims, PDFs and scanned charts into a single RADV workspace. It ingests files from connected systems, normalizes key metadata, and ties each item to the correct member, contract, and date range so teams can work from one coherent view of the record.

### Validate HCCs Before They Reach CMS

Within Martlet AI, each diagnosis is reviewed against configurable validation rules so teams can see early which conditions are well supported and which may fall short. The system checks for MEAT-style documentation, coding accuracy, date alignment and provider details, then labels each HCC with a clear validation status and the specific reasons it may be weak. This gives leaders a scalable view of documentation strength before any record is sent to CMS.

## Understand Potential RADV Error Patterns

RADV-style sampling logic is applied to the plan’s member population and the associated validation results, so leaders can see how current documentation would behave under CMS sampling.

The platform can generate projected error rates by HCC, contract or provider group and save those scenarios for comparison over time. Leaders can see how their documentation holds up under CMS sampling logic, especially as the V28 HCC model changes condition groupings and documentation expectations.

### Generate CMS-Ready Audit Packets

Martlet AI assembles audit packets for selected members by pulling in the relevant clinical pages and attaching required intake elements such as dates of service, provider identifiers, credential details and coversheets.

Each diagnosis is linked to page-level evidence and the source record, creating packet files aligned with CMS RADV expectations and supporting stronger audit readiness.

### Run RADV Review as an Exception-First Workflow

Instead of working from static lists, coders and auditors see work queues built from validation results, missing elements and sampling risk.

Flagged charts are routed to the right reviewers with the relevant pages and flags already in view, and actions on each chart are recorded. This enables an exception-first RADV workflow where the system brings high-risk records to the surface rather than teams having to search for them.

## Which Performance Metrics Prove Stronger Compliance and Reduced Exposure

Once Martlet AI is embedded in the RADV workflow, plans can see whether their audit processes are improving evidence quality and reducing internal signs of risk. The metrics that follow act as internal leading indicators that help track compliance and exposure trends, without claiming to predict specific CMS outcomes.

### Evidence Defensibility Coverage

A clear view of defensibility starts with knowing whether each submitted condition can be traced back to the right place in the chart. These metrics show how much of your risk-adjusting accuracy is supported by evidence that is both complete and easy to verify.

*   **Defensible HCC rate:** Percentage of HCCs in scope where Martlet shows MEAT-complete documentation and at least one linked page-level or line-level citation to the medical record. A higher rate indicates that more of the risk-adjusting conditions you rely on are supported to an audit-ready standard.
*   **Citation and traceability coverage:** Percentage of HCCs with explicit evidence links back to the chart pages used to support each code. Strong coverage makes it easier for internal teams, and later auditors, to confirm why a diagnosis was submitted and where the proof sits.

### Modelled RADV Risk Signal

RADV-style sampling reveals how documentation patterns behave under audit pressure and highlights where risk is gathering.

**Change in modelled error rate:** Difference between baseline and current projected error rates by HCC, contract or provider group, based on Martlet’s sampling runs and validation flags. A downward trend suggests that remediation work is reducing the proportion of diagnoses that would be vulnerable in a RADV-style sample.

### Packet Quality and Intake Readiness

Packet defects often drive losses before the clinical story is reviewed. These metrics focus on how reliably Martlet assembles evidence into CMS-ready packets so defensible diagnoses are not removed over avoidable intake issues.

*   **Internal packet defect or invalid rate:** Percentage of packets that fail Martlet’s intake checks because required elements are missing or inconsistent, such as dates of service, signatures or provider credentials. Lower defect rates indicate fewer avoidable intake issues that could remove valid diagnoses from consideration.
*   **First-pass internal QA pass rate:** Percentage of packets that pass all internal checks in Martlet on the first attempt, without extra document retrieval or edits. Higher first-pass rates point to more consistent packet quality and less back-and-forth work for teams.

### Review Efficiency

A large share of RADV delays and errors comes from time spent hunting for pages rather than reviewing the evidence itself. These metrics show how much of the workflow Martlet moves into structured queues so experts can focus on the charts that influence defensibility.

*   **Coder throughput per FTE:** Number of charts or HCCs reviewed over a given period while coders and auditors work from Martlet’s queues and evidence views. Improvements here indicate that more review work is being completed with the same staffing levels.
*   **End-to-end cycle time:** Elapsed time from chart ingestion into Martlet through validation and packet generation for a defined population. Shorter cycle times show that evidence is moving through the RADV workflow more quickly, with fewer delays between ingestion, review, and packet readiness.

## What Health Plans Gain by Adopting Audit-Ready Evidence as the Default

With Martlet AI embedded in day-to-day RADV workflows and measured through the metrics above, leaders gain a clearer view of how documentation strength and audit posture evolve over time. The practical impact becomes visible across areas that matter most to finance, risk adjustment and compliance.

### Lower Visible Clawback Exposure

Health plans can lower internal signs of clawback risk by identifying weak or unsupported HCCs sooner and using modelled error-rate trends to guide remediation. Increasing the share of diagnoses supported by MEAT-complete, traceable chart evidence strengthens defensibility even further.

### Stronger, More Defensible Compliance Posture

Compliance and audit teams work from submitted codes that link directly to the pages in the record that support them. Each condition can be rebuilt as a CMS ready packet, which makes explanations clearer and responses more consistent when audits occur.

### Faster, More Focused Review Work

Coding and review teams spend less time searching for documents and more time resolving the cases that need clinical judgment. High-volume retrieval and validation steps move into Martlet queues, leading to more charts cleared per reviewer, fewer avoidable packet defects, and shorter cycle times from ingestion to audit-ready evidence.

## Your Next Step Toward RADV Audit Readiness With Martlet AI

Martlet AI gives health plans a practical way to understand RADV exposure before CMS selects a sample. The platform links each diagnosis to traceable chart evidence, runs MEAT and coding checks at scale, assembles CMS-ready packets, and models internal error signals through mock RADV sampling.

Leaders get a clear picture of which diagnoses are truly supported and which could fall out under audit, turning RADV preparation from a reactive scramble into a steady operational process.

With Martlet in place, teams gain the ability to:

*   See documentation strength earlier in the year, rather than only after an audit notice
*   Identify which HCCs carry exposure based on modelled error signals and evidence gaps
*   Track whether defensibility and packet quality are improving through year-round metrics
*   Spend more time on clinical judgement and less time hunting for pages across systems

The result is steadier control over exposure, more traceable evidence, and a workflow that supports more predictable revenue and a clearer compliance posture.

If you want to explore how this works on your own contracts and data, you can [book a demo](https://meetings.hubspot.com/ritwik-jain?uuid=0616cd42-5ffd-4a4f-8c09-a3ccfee6dab1) with the Martlet AI team and walk through the evidence views, sampling outputs, and packet generation in detail.